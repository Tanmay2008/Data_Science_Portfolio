{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>15574012</td>\n",
       "      <td>Chu</td>\n",
       "      <td>645</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>15592531</td>\n",
       "      <td>Bartlett</td>\n",
       "      <td>822</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>15656148</td>\n",
       "      <td>Obinna</td>\n",
       "      <td>376</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>15792365</td>\n",
       "      <td>He</td>\n",
       "      <td>501</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>15592389</td>\n",
       "      <td>H?</td>\n",
       "      <td>684</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>134603.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71725.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>15767821</td>\n",
       "      <td>Bearce</td>\n",
       "      <td>528</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>102016.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80181.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>15737173</td>\n",
       "      <td>Andrews</td>\n",
       "      <td>497</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76390.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>15632264</td>\n",
       "      <td>Kay</td>\n",
       "      <td>476</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26260.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>15691483</td>\n",
       "      <td>Chin</td>\n",
       "      <td>549</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190857.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>15600882</td>\n",
       "      <td>Scott</td>\n",
       "      <td>635</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65951.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>15643966</td>\n",
       "      <td>Goforth</td>\n",
       "      <td>616</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>143129.41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64327.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>15737452</td>\n",
       "      <td>Romeo</td>\n",
       "      <td>653</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>132602.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5097.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>15788218</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>549</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14406.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>15661507</td>\n",
       "      <td>Muldrow</td>\n",
       "      <td>587</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158684.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>15568982</td>\n",
       "      <td>Hao</td>\n",
       "      <td>726</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54724.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
       "0           1    15634602   Hargrave          619    France  Female   42   \n",
       "1           2    15647311       Hill          608     Spain  Female   41   \n",
       "2           3    15619304       Onio          502    France  Female   42   \n",
       "3           4    15701354       Boni          699    France  Female   39   \n",
       "4           5    15737888   Mitchell          850     Spain  Female   43   \n",
       "5           6    15574012        Chu          645     Spain    Male   44   \n",
       "6           7    15592531   Bartlett          822    France    Male   50   \n",
       "7           8    15656148     Obinna          376   Germany  Female   29   \n",
       "8           9    15792365         He          501    France    Male   44   \n",
       "9          10    15592389         H?          684    France    Male   27   \n",
       "10         11    15767821     Bearce          528    France    Male   31   \n",
       "11         12    15737173    Andrews          497     Spain    Male   24   \n",
       "12         13    15632264        Kay          476    France  Female   34   \n",
       "13         14    15691483       Chin          549    France  Female   25   \n",
       "14         15    15600882      Scott          635     Spain  Female   35   \n",
       "15         16    15643966    Goforth          616   Germany    Male   45   \n",
       "16         17    15737452      Romeo          653   Germany    Male   58   \n",
       "17         18    15788218  Henderson          549     Spain  Female   24   \n",
       "18         19    15661507    Muldrow          587     Spain    Male   45   \n",
       "19         20    15568982        Hao          726    France  Female   24   \n",
       "\n",
       "    Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0        2       0.00              1          1               1   \n",
       "1        1   83807.86              1          0               1   \n",
       "2        8  159660.80              3          1               0   \n",
       "3        1       0.00              2          0               0   \n",
       "4        2  125510.82              1          1               1   \n",
       "5        8  113755.78              2          1               0   \n",
       "6        7       0.00              2          1               1   \n",
       "7        4  115046.74              4          1               0   \n",
       "8        4  142051.07              2          0               1   \n",
       "9        2  134603.88              1          1               1   \n",
       "10       6  102016.72              2          0               0   \n",
       "11       3       0.00              2          1               0   \n",
       "12      10       0.00              2          1               0   \n",
       "13       5       0.00              2          0               0   \n",
       "14       7       0.00              2          1               1   \n",
       "15       3  143129.41              2          0               1   \n",
       "16       1  132602.88              1          1               0   \n",
       "17       9       0.00              2          1               1   \n",
       "18       6       0.00              1          0               0   \n",
       "19       6       0.00              2          1               1   \n",
       "\n",
       "    EstimatedSalary  Exited  \n",
       "0         101348.88       1  \n",
       "1         112542.58       0  \n",
       "2         113931.57       1  \n",
       "3          93826.63       0  \n",
       "4          79084.10       0  \n",
       "5         149756.71       1  \n",
       "6          10062.80       0  \n",
       "7         119346.88       1  \n",
       "8          74940.50       0  \n",
       "9          71725.73       0  \n",
       "10         80181.12       0  \n",
       "11         76390.01       0  \n",
       "12         26260.98       0  \n",
       "13        190857.79       0  \n",
       "14         65951.65       0  \n",
       "15         64327.26       0  \n",
       "16          5097.67       1  \n",
       "17         14406.41       0  \n",
       "18        158684.81       0  \n",
       "19         54724.03       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 3:13].values #Independent variable 3 to 12 is responsible for leaving any customer\n",
    "y = dataset.iloc[:, 13].values # 13 is the independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tp040576\\.conda\\envs\\AntonioEnv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\tp040576\\.conda\\envs\\AntonioEnv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Encoding categorical Data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder() \n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])# Encoding categorical Variable Geography\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])# Encoding categorical Variable Gender\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])#Passing Geography Variable to create a Dummy variable\n",
    "#onehotencoder = OneHotEncoder(ColumnTransformer = [1])#Passing Geography Variable to create a Dummy variable\n",
    "X = onehotencoder.fit_transform(X).toarray()#Create a dummy variable\n",
    "X = X[:, 1:] #Remove first dummy variable or first coulmn to save from dummy variable trap\n",
    "#Splitting the dataset into the training and test data set\n",
    "#from sklearn.cross_validation import train_test_split#No module named 'sklearn.cross_validation' as 'cross_validation' replaced bymodel_selection \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "#Feature scaling compulsury for ANN to ease the calculation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above date now are well pre processed now we are statrt building ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Lets make ANN\n",
    "#Import the Keras Library\n",
    "import tensorflow\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import krras module from keras library and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from keras.models import Sequential #Sequential module requires to initialize the neural network\n",
    "    from keras.layers import Dense #Dense module requires to build layer of our ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the ANN\n",
    "classifier = Sequential()#classifier object is the sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tp040576\\.conda\\envs\\AntonioEnv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\tp040576\\.conda\\envs\\AntonioEnv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tp040576\\.conda\\envs\\AntonioEnv\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tp040576\\.conda\\envs\\AntonioEnv\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Adding the 2nd hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tp040576\\.conda\\envs\\AntonioEnv\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tp040576\\.conda\\envs\\AntonioEnv\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\tp040576\\.conda\\envs\\AntonioEnv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\tp040576\\.conda\\envs\\AntonioEnv\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1240/8000 [===>..........................] - ETA: 0s - loss: 0.4196 - acc: 0.8234"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tp040576\\.conda\\envs\\AntonioEnv\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.3934 - acc: 0.8380\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.3934 - acc: 0.8394\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 0.3931 - acc: 0.8381\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.3926 - acc: 0.8381\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 0.3924 - acc: 0.8382\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.3929 - acc: 0.8407\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.3925 - acc: 0.8402\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 0.3925 - acc: 0.8372\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 0.3921 - acc: 0.8391\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 0.3916 - acc: 0.8381\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.3919 - acc: 0.8386 0s - loss: 0.3999 - a\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 0.3916 - acc: 0.8384\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 0.3913 - acc: 0.8385\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 0.3914 - acc: 0.8397\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 0.3915 - acc: 0.8382\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3910 - acc: 0.8392\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.3908 - acc: 0.8380\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 146us/step - loss: 0.3913 - acc: 0.8399\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.3902 - acc: 0.8399\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.3908 - acc: 0.8392\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 0.3913 - acc: 0.8374\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 0.3908 - acc: 0.8384\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3907 - acc: 0.8394\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 0.3904 - acc: 0.8387\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 0.3909 - acc: 0.8390\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3906 - acc: 0.8387\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.3908 - acc: 0.8377\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 0.3906 - acc: 0.8384\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 0.3905 - acc: 0.8379\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 0.3906 - acc: 0.8380\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 0.3900 - acc: 0.8406\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 307us/step - loss: 0.3900 - acc: 0.8401\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 0.3903 - acc: 0.8405\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 238us/step - loss: 0.3904 - acc: 0.8379\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.3905 - acc: 0.8387\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3903 - acc: 0.8400\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 150us/step - loss: 0.3901 - acc: 0.8399\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 0.3904 - acc: 0.8392\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3899 - acc: 0.8402\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 0.3897 - acc: 0.8396\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 2s 285us/step - loss: 0.3900 - acc: 0.8379\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.3897 - acc: 0.8390\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 0.3898 - acc: 0.8399\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.3898 - acc: 0.8404\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 0.3895 - acc: 0.8414\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3898 - acc: 0.8405\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.3894 - acc: 0.8394\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 0.3890 - acc: 0.8396\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: 0.3897 - acc: 0.8377\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 0.3893 - acc: 0.8407\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 0.3897 - acc: 0.8389\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 0.3894 - acc: 0.8386\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 0.3894 - acc: 0.8407\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 146us/step - loss: 0.3891 - acc: 0.8395\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 0.3890 - acc: 0.8402\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.3891 - acc: 0.8400\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.3887 - acc: 0.8404 1s - loss: 0.\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.3887 - acc: 0.8415\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 0.3889 - acc: 0.8389\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3885 - acc: 0.8406\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3885 - acc: 0.8415\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.3884 - acc: 0.8405 0s - loss: 0.3882 - \n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.3886 - acc: 0.8396\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.3884 - acc: 0.8400\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.3885 - acc: 0.8405\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 0.3880 - acc: 0.8416\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 0.3877 - acc: 0.8409\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.3882 - acc: 0.8405\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.3883 - acc: 0.8390 0s - loss: 0.3862 - acc\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.3878 - acc: 0.8405\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 0.3880 - acc: 0.8394\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 0.3879 - acc: 0.8422\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 0.3882 - acc: 0.8406 0s - loss: 0.3853 - acc: \n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.3880 - acc: 0.8395\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.3877 - acc: 0.8406\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.3878 - acc: 0.8402\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 143us/step - loss: 0.3876 - acc: 0.8386\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.3876 - acc: 0.8406\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.3879 - acc: 0.8420\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.3877 - acc: 0.8397\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 0.3874 - acc: 0.8427\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 0.3879 - acc: 0.8402\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 0.3877 - acc: 0.8404\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.3874 - acc: 0.8411\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 0.3875 - acc: 0.8407\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 0.3872 - acc: 0.8374\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 0.3871 - acc: 0.8412\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 0.3871 - acc: 0.8419\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.3872 - acc: 0.8404\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 0.3869 - acc: 0.8401\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3870 - acc: 0.8419\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.3870 - acc: 0.8407\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 144us/step - loss: 0.3871 - acc: 0.8415\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.3869 - acc: 0.8411\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3868 - acc: 0.8417 2s - l\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.3867 - acc: 0.8404\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.3867 - acc: 0.8405\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 0.3865 - acc: 0.8429\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 0.3867 - acc: 0.8424\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 0.3864 - acc: 0.8434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25d66e0e4c8>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the ANN to traininnh set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting the test set result\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1490,  105],\n",
       "       [ 217,  188]], dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm  # Accuracy of cm is 83.9% and it is pretty good"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
